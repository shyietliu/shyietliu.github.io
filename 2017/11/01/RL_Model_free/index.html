<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="RL," />










<meta name="description" content="[TOC] Model-Free Methods Previously we assume that someone tells us the exact MDP so that we can solve it. However in practical, sometimes it&apos;s difficult to know the MDP. In this case, Model-Based me">
<meta name="keywords" content="RL">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning 4 - Model-Free Methods">
<meta property="og:url" content="http://yoursite.com/2017/11/01/RL_Model_free/index.html">
<meta property="og:site_name" content="Willand">
<meta property="og:description" content="[TOC] Model-Free Methods Previously we assume that someone tells us the exact MDP so that we can solve it. However in practical, sometimes it&apos;s difficult to know the MDP. In this case, Model-Based me">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/pseudotmp3.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/pseudotmp5.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/pseudotmp6.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/pseudotmp7.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/pseudotmp8.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/pseudotmp9.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/figtmp36.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/figtmp40.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/numeqtmp36.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/imgtmp15.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/figtmp42.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/numeqtmp40.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/pseudotmp12.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/imgtmp59.png">
<meta property="og:image" content="http://incompleteideas.net/book/ebook/pseudotmp13.png">
<meta property="og:updated_time" content="2018-05-20T23:37:28.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reinforcement Learning 4 - Model-Free Methods">
<meta name="twitter:description" content="[TOC] Model-Free Methods Previously we assume that someone tells us the exact MDP so that we can solve it. However in practical, sometimes it&apos;s difficult to know the MDP. In this case, Model-Based me">
<meta name="twitter:image" content="http://incompleteideas.net/book/ebook/pseudotmp3.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/11/01/RL_Model_free/"/>





  <title>Reinforcement Learning 4 - Model-Free Methods | Willand</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Willand</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/01/RL_Model_free/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Shyiet Liu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://www.wallpapers13.com/wp-content/uploads/2017/06/Dota-2-Pictures-gallery-Invoker-Magic-fighter-Digital-Art-Wallpaper-HD-1920x1080-1366x768.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Willand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Reinforcement Learning 4 - Model-Free Methods</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-01T20:10:33+00:00">
                2017-11-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/reinforcement-learning/" itemprop="url" rel="index">
                    <span itemprop="name">reinforcement learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">



      
      

      
        <p><link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet"></p>
<p>[TOC]</p>
<h1 id="model-free-methods">Model-Free Methods</h1>
<p>Previously we assume that someone tells us the exact MDP so that we can solve it. However in practical, sometimes it's difficult to know the MDP. In this case, Model-Based methods are not acceptable (for instance, we don't know the transition probability, we can't compute value function by dynamic programming.)</p>
<p>An alternative is Model-Free methods, which do not rely on the knowledge of that unknown MDP.</p>
<p>Model-Free methods still need to run policy evaluation and policy improvement repeatedly to reach convergence (optimal value function and optimal policy, sometimes suboptimal). But in Model-Free scene they usually called <strong>prediction</strong> (policy evaluation ) and <strong>control</strong> (policy improvement) respectively. There are 2 mainstream methods for model-free methods: Monte-Carlo (MC) and Temporal Difference (TD) and they both have prediction and control algorithm.</p>
<p>For prediction, we still want to obtain the value function of states. As we are not able to update the state value by backup, we estimate the value function. Generally, to get value function, MC and TD use the following equation: <span class="math display">\[
\begin{equation}
\begin{split}
V(s)&amp;=V(s)+\alpha \delta\\
&amp;=V(s)+\alpha[T-V(s)]\\
\end{split}
\end{equation}
\]</span> where <span class="math inline">\(\delta\)</span> called error, <span class="math inline">\(T\)</span> called target and <span class="math inline">\(\alpha\)</span> is step-size. For now we assume we know <span class="math inline">\(T\)</span> somehow.</p>
<p>For control, we use the similar idea to policy improvement with a little adjustment.</p>
<h2 id="monte-carlo">4.1 Monte-Carlo</h2>
<h3 id="monte-carlo-prediction">4.1.1 Monte-Carlo Prediction</h3>
<p>Recall the definition of return and value function: <span class="math display">\[
V^\pi(s)=E\{G_t|s_t=s\}
\]</span> Monte-Carlo method based on a fact that <span class="math display">\[
V^\pi(s)=\lim_{N\rightarrow \infty}\frac{S(s)}{N(s)}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1)
\]</span> where <span class="math inline">\(S(s)=\sum_{N(s)} G_t​\)</span>, <span class="math inline">\(N(s)​\)</span> is the time we visit the specific state <span class="math inline">\(s​\)</span>.</p>
<p>MC method will sample a trajectory under the given policy, we count <span class="math inline">\(N(s)\)</span> according to that trajectory (usually called episode). Theoratical details can be found in Sutton's book.</p>
<p><strong>In MC, we must assume we have a simulator that helps us generate the episode. Put it another way, every time we generate a trajectory, actually we assume someone walks in the world until the absorbing (terminate) state and tells us the reward in that trajectory. So MC can only deal with the problem having absorbing state.</strong></p>
<h4 id="first-visit-mc-prediction">First visit MC prediction</h4>
<p>We use the average return of the episode generate from that state as depicted in equation (1). For first visit MC prediciton, we increase <span class="math inline">\(N(s)\)</span> by 1 when we <strong>first</strong> meet the state <span class="math inline">\(s\)</span>. Even if state <span class="math inline">\(s\)</span> appears many times in that episode, we still add 1 to <span class="math inline">\(N(s)\)</span>.</p>
<p>The algorithm from Sutton's book:</p>
<p><img src="http://incompleteideas.net/book/ebook/pseudotmp3.png" width="400"></p>
<p>The updating of value function also can be simply expressed as: <span class="math display">\[
V(s)=V(s)+\alpha[G_t  - V(s)]
\]</span> Where <span class="math inline">\(\alpha\)</span> is step-size that replaces the average operation, <span class="math inline">\(G_t\)</span> is the return.</p>
<h4 id="every-visit-mc-prediction">Every visit MC prediction</h4>
<p>In every visit situation, we increase <span class="math inline">\(N(s)\)</span> every time we meet <span class="math inline">\(s\)</span>.</p>
<h4 id="an-example">An example</h4>
<p>Assume we have 2 episode: <span class="math display">\[
A,1,B,1,A,0,C
\]</span></p>
<p><span class="math display">\[
B,0,A,1,A,0,C
\]</span></p>
<p>If we use first visit MC to evaluate <span class="math inline">\(V(A)\)</span>, we get: <span class="math display">\[
V(A)=\frac{\text{Accumulated Return}}{\text{The time we visit state A}}=\frac{(1+1)+(1)}{2}=1.5
\]</span> For every visit MC: <span class="math display">\[
V(A)=\frac{\text{Accumulated Return}}{\text{The time we visit state A}}=\frac{(1+1)+0+1+0}{1+1+1+1}=0.75
\]</span></p>
<h3 id="monte-carlo-control">4.1.2 Monte-Carlo Control</h3>
<p>For now, we already now how to estimate a value function, we want to improve the policy by the value function. That's what MC control does.</p>
<p>To achieve control, we need estimate action-value function rather than state-value function so that we can select the best action at given state. <span class="math display">\[
\pi(s)&#39;=argmax_a(Q(s,a))
\]</span></p>
<h4 id="onoff-policy-control">4.1.2.1 On/Off-policy control</h4>
<p>There are 2 model-free control types, On-policy and Off-policy. On-policy means the policy we want to improve is the policy we are following (by 'following' I mean the policy we use to generate episode.) Off-policy means the we are going to improve is different from the policy we are following.</p>
<h4 id="exploring-starts">4.1.2.2 Exploring starts</h4>
<p>One problem we may encounter is the simulator can't explore the whole world, for instance the agent can reach 4 states <span class="math inline">\(A,B,C,D\)</span>, where <span class="math inline">\(D\)</span> is the absorbing state and the agent can move to any state. The episodes generated by simulator may never contain state <span class="math inline">\(A\)</span>, so we can not get valid estimation and loss information. To deal with that, we may set a probability <span class="math inline">\(\epsilon=0.10\)</span>. With that probability the agent won't follow the policy but randomly select the next move. That can ensure that the agent explore the whole world. If we apply this on greedy selection, we get &quot;<span class="math inline">\(\epsilon\)</span>-greedy&quot;.</p>
<h4 id="on-policy-mc-control">4.1.2.3 On-policy MC Control</h4>
<p><img src="http://incompleteideas.net/book/ebook/pseudotmp5.png" width="480"></p>
<h4 id="off-policy-mc-control">4.1.2.4 Off-policy MC Control</h4>
<p><img src="http://incompleteideas.net/book/ebook/pseudotmp6.png" width="500"></p>
<h2 id="temporal-difference">4.2 Temporal Difference</h2>
<h3 id="temporal-difference-prediction">4.2.1 Temporal Difference Prediction</h3>
<h4 id="td0">4.2.1.1 TD(0)</h4>
<p>MC prediction uses the difference between return and value function <span class="math inline">\(G_t-V(s_t)\)</span> to iterative, it requires finite state and needs to generate a whole episode. While TD only look one step ahead to update value function. <span class="math display">\[
V(s)=V(s)+\alpha[r_{ss&#39;}+\gamma V(s&#39;)-V(s)]
\]</span> This is called TD(0) as we look ahead for one step.</p>
<p><img src="http://incompleteideas.net/book/ebook/pseudotmp7.png" width="450"></p>
<h4 id="bootstrap">4.2.1.2 Bootstrap</h4>
<p>Note that in MC, the target is the return and we can obtain the exact value of it by running the simulator. However for TD, we need update the value function based on <span class="math inline">\(V(s&#39;)\)</span>, which is an <strong>estimated value</strong>. The method that estimate a value using another estimation called <strong>bootstrap</strong>.</p>
<h3 id="temporal-difference-control">4.2.2 Temporal Difference Control</h3>
<p>The control of TD is similar with MC, but TD control is widely used so people give them other names based on their characters.</p>
<h4 id="on-policy-td-control-sarsa">4.2.2.1 On-policy TD Control — SARSA</h4>
<p>The basic idea of SARSA is <span class="math display">\[
Q(s,a)=Q(s,a)+\alpha[r+\gamma Q(s&#39;,a&#39;)-Q(s,a)]
\]</span> If you draw a backup diagram, we are using <u>a state <strong><span class="math inline">\(s\)</span></strong>, an action <span class="math inline">\(a\)</span>, a reward <span class="math inline">\(r\)</span>, a consecutive state <span class="math inline">\(s&#39;\)</span> and the corresponding action <span class="math inline">\(a&#39;\)</span></u> to update the Q function, that's why it called <u>SARSA</u>.</p>
<p>SARSA algorithm:</p>
<p><img src="http://incompleteideas.net/book/ebook/pseudotmp8.png" width="500"></p>
<h4 id="off-policy-td-control-q-learning">4.2.2.2 Off-policy TD Control — Q-Learning</h4>
<p>One of the most important breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as <em>Q-learning</em> (Watkins, 1989). Its simplest form, <em>one-step Q-learning</em>, is defined by <span class="math display">\[
Q(s,a)=Q(s,a)+\alpha[r+\gamma  \text{max}_a(Q(s&#39;,a))-Q(s,a)]
\]</span> <strong>Q-learning Algorithm:</strong></p>
<p><img src="http://incompleteideas.net/book/ebook/pseudotmp9.png" width="500"></p>
<h2 id="eligibility-traces">4.3 Eligibility Traces</h2>
<h3 id="n-step-td-prediction">4.3.1 n-step TD prediction</h3>
<p>For TD(0), we just look head one step, can we use more state in that episode? Yes we can.</p>
<p><img src="http://incompleteideas.net/book/ebook/figtmp36.png" width="400"></p>
<p>If we use n-step, we only need to change the target in the value function update equation. The n-step target is: <span class="math display">\[
G_t^{(n)}=r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+...+\gamma^{(n-1)}r_{t+n}+\gamma^nV_t(s_{t+n})
\]</span> To update the value function, we just need <span class="math display">\[
V(s_t) = V(s_t)+\alpha[G_t^{(n)}-V(s_t)]
\]</span></p>
<h3 id="tdλ-prediction">4.3.2 TD(λ)-prediction</h3>
<h4 id="forward-view">4.3.2.1 Forward view</h4>
<p>A backup that averages simpler component backups in this way is called a <em>complex backup</em>. TD(λ) is a special way to averaging n-step backups by using: <span class="math display">\[
G_t^{\lambda}=(1-\lambda)\sum_{n-1}^{\infty} \lambda^{n-1}G_t^{(n)}
\]</span> We still update the value function by <span class="math display">\[
V(s_t) = V(s_t)+\alpha[G_t^{(n)}-V(s_t)]
\]</span> Forward view:</p>
<p><img src="http://incompleteideas.net/book/ebook/figtmp40.png" width="600"></p>
<p>But note that <span class="math inline">\(\lambda\)</span>-step is NOT typically called TD(<span class="math inline">\(\lambda\)</span>).</p>
<h4 id="backward-view">4.3.2.2 Backward view</h4>
<p>To get backward view TD(λ), we first need to define a new term: eligibility traces</p>
<p><img src="http://incompleteideas.net/book/ebook/numeqtmp36.png" width="250"></p>
<p>for each unique state, there is a corresponding eligibility trace <span class="math inline">\(e_t(s)\)</span> indicating how 'frequently' we visit a state.</p>
<p><img src="http://incompleteideas.net/book/ebook/imgtmp15.png" width="450"></p>
<p>Recall the general value function update equation: <span class="math display">\[
\begin{equation}
\begin{split}
V(s)&amp;=V(s)+\Delta V(s)\\
&amp;=V(s)+\alpha \delta\\
&amp;=V(s)+\alpha[T-V(s)]\\
\end{split}
\end{equation}
\]</span> For backview TD(λ), we adjust that equation by <span class="math display">\[
\Delta V(s)=\alpha \delta_t e_t(s) \ \ \ \ \ \ \ \ \ \text{for all states} \ s
\]</span> Backward view:</p>
<p><img src="http://incompleteideas.net/book/ebook/figtmp42.png" width="450"></p>
<h4 id="equivelence-of-backward-and-forward-view">4.3.2.3 Equivelence of Backward and Forward view</h4>
<p>The forward and backward views is <strong>equivalent</strong> under certain constraints, that can be found <a href="http://incompleteideas.net/book/ebook/node76.html" target="_blank" rel="external">here</a>.</p>
<h3 id="sarsaλ">4.3.3 SARSA(λ)</h3>
<p>The eligibility trace version of SARAS (On-policy control) called SARSA(λ).</p>
<p>Similarly, we define</p>
<p><img src="http://incompleteideas.net/book/ebook/numeqtmp40.png" width="450"></p>
<p>The updating of action-value function is <span class="math display">\[
\begin{equation}
\begin{split}
Q_{t+1}(s,a)&amp;=Q_t(s,a) + \alpha \delta_t e_t(s,a) \\
\end{split}
\end{equation}
\]</span> Where <span class="math display">\[
\delta_t= \alpha[r+\gamma Q(s&#39;,a&#39;)-Q(s,a)]
\]</span> <strong>SARSA(λ) algorithm:</strong></p>
<p><img src="http://incompleteideas.net/book/ebook/pseudotmp12.png" width="500"></p>
<h3 id="qλ">4.3.4 Q(λ)</h3>
<p>we call them <em>Watkins's Q</em>(λ) and <em>Peng's Q</em>(λ), after the researchers who first proposed them. Here we describe Watkins's Q(λ) first.</p>
<h4 id="watkinss-qλ">4.3.4.1 Watkins's Q(λ)</h4>
<p>Due to Q-learning's off-policy nature, we revise eligibility traces</p>
<p><img src="http://incompleteideas.net/book/ebook/imgtmp59.png" width="500"></p>
<p>where <span class="math inline">\(\mathcal{I}_{xy}\)</span> is the indicator function, if <span class="math inline">\(x=y\)</span>, that will be <span class="math inline">\(1\)</span> otherwise <span class="math inline">\(0\)</span>. The eligibility traces are used just as in SARSA(λ), except that they are set to zero whenever an exploratory (nongreedy, not the 'best' choice) action is taken.</p>
<p>We update Q-function as usual <span class="math display">\[
\begin{equation}
\begin{split}
Q_{t+1}(s,a)&amp;=Q_t(s,a) + \alpha \delta_t e_t(s,a) \\
\end{split}
\end{equation}
\]</span> where <span class="math display">\[
\delta_t=r+\gamma  \text{max}_a[Q_t(s&#39;,a)]-Q_t(s,a)
\]</span> <strong>Watkins's Q(λ) algorithm:</strong></p>
<p><img src="http://incompleteideas.net/book/ebook/pseudotmp13.png" width="500"></p>
<h4 id="pengs-qλ">4.3.4.2 Peng's Q(λ)</h4>
<p>Unfortunately, cutting off traces every time an exploratory action is taken loses much of the advantage of using eligibility traces. If exploratory actions are frequent, as they often are early in learning, then only rarely will backups of more than one or two steps be done, and learning may be little faster than one-step Q-learning. <a href="http://incompleteideas.net/book/ebook/node78.html" target="_blank" rel="external">Peng's Q(λ)</a> is an alternative version of Q(λ) meant to remedy this.</p>

      

      <div>
        
          <div>
    
        <div style="text-align:center;color: #636363;font-size:14px;letter-spacing: 10px">本文结束啦<i class="fa fa-bell"></i>感谢您的阅读</div>
    
</div>

        
      </div>


    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/RL/" rel="tag"># RL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/01/RL_v_function/" rel="next" title="Reinforcement Learning 2 - Value function">
                <i class="fa fa-chevron-left"></i> Reinforcement Learning 2 - Value function
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/02/08/RL_FA/" rel="prev" title="Reinforcement Learning 5 - Function Approximation">
                Reinforcement Learning 5 - Function Approximation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>




          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="http://www.wallpapers13.com/wp-content/uploads/2017/06/Dota-2-Pictures-gallery-Invoker-Magic-fighter-Digital-Art-Wallpaper-HD-1920x1080-1366x768.jpg"
                alt="Shyiet Liu" />
            
              <p class="site-author-name" itemprop="name">Shyiet Liu</p>
              <p class="site-description motion-element" itemprop="description">Which hearts more</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#model-free-methods"><span class="nav-text">Model-Free Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#monte-carlo"><span class="nav-text">4.1 Monte-Carlo</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#monte-carlo-prediction"><span class="nav-text">4.1.1 Monte-Carlo Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#first-visit-mc-prediction"><span class="nav-text">First visit MC prediction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#every-visit-mc-prediction"><span class="nav-text">Every visit MC prediction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#an-example"><span class="nav-text">An example</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#monte-carlo-control"><span class="nav-text">4.1.2 Monte-Carlo Control</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#onoff-policy-control"><span class="nav-text">4.1.2.1 On/Off-policy control</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#exploring-starts"><span class="nav-text">4.1.2.2 Exploring starts</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#on-policy-mc-control"><span class="nav-text">4.1.2.3 On-policy MC Control</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#off-policy-mc-control"><span class="nav-text">4.1.2.4 Off-policy MC Control</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#temporal-difference"><span class="nav-text">4.2 Temporal Difference</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#temporal-difference-prediction"><span class="nav-text">4.2.1 Temporal Difference Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#td0"><span class="nav-text">4.2.1.1 TD(0)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bootstrap"><span class="nav-text">4.2.1.2 Bootstrap</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#temporal-difference-control"><span class="nav-text">4.2.2 Temporal Difference Control</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#on-policy-td-control-sarsa"><span class="nav-text">4.2.2.1 On-policy TD Control — SARSA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#off-policy-td-control-q-learning"><span class="nav-text">4.2.2.2 Off-policy TD Control — Q-Learning</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#eligibility-traces"><span class="nav-text">4.3 Eligibility Traces</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#n-step-td-prediction"><span class="nav-text">4.3.1 n-step TD prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tdλ-prediction"><span class="nav-text">4.3.2 TD(λ)-prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#forward-view"><span class="nav-text">4.3.2.1 Forward view</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#backward-view"><span class="nav-text">4.3.2.2 Backward view</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#equivelence-of-backward-and-forward-view"><span class="nav-text">4.3.2.3 Equivelence of Backward and Forward view</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sarsaλ"><span class="nav-text">4.3.3 SARSA(λ)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#qλ"><span class="nav-text">4.3.4 Q(λ)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#watkinss-qλ"><span class="nav-text">4.3.4.1 Watkins's Q(λ)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pengs-qλ"><span class="nav-text">4.3.4.2 Peng's Q(λ)</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shyiet Liu</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
