<!DOCTYPE html>
<html lang="en">
  <!-- Head tag -->
  <head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Title -->
  
  <title>First Night - Willand!</title>

  <!--Favicon-->
  <link rel="icon" href="favicon/favicon.ico">

  <!--Description-->
  
      <meta name="description" content="Dependency syntax and parsing$$aa$$考虑另一种语法形式：依存语法。
在这种语法下，我们不再需要短语成分和短语结构。取而代之的是基于句法结构的词与词之间的关系。这种关系是二元的，有向的。
下图展示了一个依存语法分析的结构。
Here we present anothe">
  

  <!--Author-->
  
      <meta name="author" content="Shyiet Liu">
  

  <!-- Pure CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
  <link href="https://fonts.googleapis.com/css?family=Crimson+Text|Open+Sans:300,800" rel="stylesheet">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/styles.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <!-- Google Analytics --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>


  <body>
  	<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container-fluid navbar-container m-sm-5">
      <!-- Header -->
      <nav class="navbar navbar-toggleable-sm navbar-light px-1 py-3 my-3 mb-sm-5">
  <a class="navbar-brand ml-2" href="/">Willand!</a>
  <button class="navbar-toggler navbar-toggler-right py-2" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse text-center" id="navbarCollapse">
    <ul class="navbar-nav ml-auto my-auto">
      
        <li class="nav-item">
          <a class="nav-link" href="/about">About</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="/contact">Contact</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://www.instagram.com">Instagram</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://www.linkedin.com">LinkedIn</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://www.facebook.com">Facebook</a>
        </li>
      
    </ul>
    <hr class="hidden-md-up" />
  </div>
</nav>


  		<div class="row">
  			<div class="col-12 mb-4">
  <img class="img-fluid project-img" src="/" alt="First Night">
</div>
<div class="col-lg-4 col-12 pt-3 px-4 pr-lg-5">
  <h1>First Night</h1>
</div>
<div class="col-lg-8 col-12 pt-lg-3 mb-4 pl-lg-5 px-lg-0 px-4 portfolio-content">
  <h1 id="Dependency-syntax-and-parsing"><a href="#Dependency-syntax-and-parsing" class="headerlink" title="Dependency syntax and parsing"></a>Dependency syntax and parsing</h1><p>$$<br>aa<br>$$<br>考虑另一种语法形式：<strong>依存语法</strong>。</p>
<p>在这种语法下，我们不再需要短语成分和短语结构。取而代之的是基于句法结构的词与词之间的关系。这种关系是二元的，有向的。</p>
<p>下图展示了一个依存语法分析的结构。</p>
<p>Here we present another family of grammar formalisms called <strong>dependency grammars</strong>, In these formalisms, phrasal constituents and phrase-structure rules do not play a direct role. Instead, the syntactic structure of a sentence is described solely in terms of the words (or lemmas) in a sentence and an associated set of directed binary grammatical relations that hold among the words.</p>
<p>The following graph demonstrates the structure of dependency-style analysis.</p>
<p><img src="/Users/shyietliu/Desktop/屏幕快照 2017-10-23 下午8.46.08.png" alt="屏幕快照 2017-10-23 下午8.46.08">            </p>
<p>Relations among the words are illustrated above the sentence with directed, labeled arcs from heads to dependents. We call this a <strong>typed dependency structure</strong> because the labels are drawn from a fixed inventory of grammatical relations.<br>上图中词与词的关系用一个带标签的有向箭头表示。像上图这样的结构被称为<strong>typed依存结构</strong>，因为所有的标签都来自于一个固定的语法关系库。</p>
<h2 id="1-Dependency-syntax"><a href="#1-Dependency-syntax" class="headerlink" title="1.    Dependency syntax"></a>1.    Dependency syntax</h2><hr>
<h3 id="1-1-Dependency-syntax"><a href="#1-1-Dependency-syntax" class="headerlink" title="1.1    Dependency syntax"></a>1.1    Dependency syntax</h3><p>Dependency relation consists of <strong>a head</strong> and <strong>a dependent</strong>.</p>
<p><u>The head word of a constituent</u> was the central organizing word of a larger constituent (e.g, the primary noun in a noun phrase, or verb in a verb phrase).</p>
<p><u>The remaining words in the constituent</u> are either direct, or indirect, dependents of their head.</p>
<p><img src="/Users/shyietliu/Desktop/屏幕快照 2017-10-23 下午9.44.35.png" alt="屏幕快照 2017-10-23 下午9.44.35"></p>
<p>In addition to specifying the head-dependent pairs, <strong>dependency grammars</strong> allowus to further <u>classify the kinds of grammatical relations, or grammatical function, in terms of the role that the dependent plays with respect to its head.</u> Familiar notions such as subject, direct object and indirect object are among the kind of relations we have in mind.</p>
<p>​            <img src="/Users/shyietliu/Desktop/屏幕快照 2017-10-23 下午9.48.08.png" alt="屏幕快照 2017-10-23 下午9.48.08"></p>
<p>In their most general form, the dependency structures we’re discussing are simply directed graphs. That is, structures $G = (V, A)$ consisting of a set of vertices $V$ , and a set of ordered pairs of vertices $A$, which we’ll refer to as arcs.</p>
<p>The elements in the sentence is $V$, while the arcs that represent the head-dependent and grammatical function relationships between the elements in $V$.</p>
<h3 id="1-2-dependency-tree"><a href="#1-2-dependency-tree" class="headerlink" title="1.2    dependency tree"></a>1.2    dependency tree</h3><p>A valid dependency tree for a sentence requires:</p>
<ol>
<li>There is a single designated root node that has no incoming arcs.</li>
<li>Withtheexceptionoftherootnode,eachvertexhasexactlyoneincomingarc.</li>
<li>There is a unique path from the root node to each vertex in V .</li>
</ol>
<p><img src="/Users/shyietliu/Desktop/屏幕快照 2017-10-23 下午10.20.47.png" alt="屏幕快照 2017-10-23 下午10.20.47"></p>
<h2 id="2-Transform-constituency-→-dependency-parse"><a href="#2-Transform-constituency-→-dependency-parse" class="headerlink" title="2.    Transform constituency → dependency parse"></a>2.    Transform constituency → dependency parse</h2><hr>
<p>The most widely used English <u>dependency treebanks were automatically derived from phrase- structure treebanks</u> through the use of <strong>head-finding rules</strong>.</p>
<h3 id="2-1-Head-finding-rules-to-build-Lexicalized-Constituency-Parse"><a href="#2-1-Head-finding-rules-to-build-Lexicalized-Constituency-Parse" class="headerlink" title="2.1    Head-finding rules to build Lexicalized Constituency Parse"></a>2.1    Head-finding rules to build Lexicalized Constituency Parse</h3><p><strong>What’s head?</strong></p>
<p>The head is the word in the phrase that is grammatically the most important.</p>
<p><strong>How we get head given a parse tree?</strong></p>
<p>step1.    choose head daughter</p>
<p>For the generation of a parse tree with head, each CFG rule must be augmented to identify one right-side constituent to be the head daughter. i.e. for the CFG rule: $A\to \alpha \beta \gamma$, the tree should decide a head daughter for $A$  from $\alpha \beta \gamma$.</p>
<p>The headword for a node is then set to the headword of its head daughter.</p>
<p>Heads are passed up the parse tree; thus, <u>each non-terminal</u> in a parse tree is annotated with a single <u>word</u>, which is its <u>lexical head</u>.</p>
<p><img src="/Users/shyietliu/Desktop/屏幕快照 2017-10-23 下午10.54.57.png" alt="111"></p>
<h3 id="2-2-Convert-constituency-tree-to-dependency-tree"><a href="#2-2-Convert-constituency-tree-to-dependency-tree" class="headerlink" title="2.2    Convert constituency tree to dependency tree"></a>2.2    Convert constituency tree to dependency tree</h3><p>Assume we have a <strong>Lexicalized Constituency Parse tree</strong> (i.e. the constituency parsing tree with head like the graph above), we then could convert this tree to a dependency tree.</p>
<p><img src="/Users/shyietliu/Pictures/ANLP-dependency/图片 1.png" alt="图片 1"></p>
<p>We can finish the convert process by following these steps:</p>
<p>step1.    remove the phrasal categories from the lexicalized constituency parse tree.</p>
<p>step2.    remove the (duplicated) terminals</p>
<p>Step3.    collapse chains of duplicates</p>
<p><img src="/Users/shyietliu/Pictures/ANLP-dependency/图片 2.png" alt="图片 2"></p>
<p>Now we can convert a  constituency tree to a dependency tree.</p>
<h3 id="2-3-Define-the-head-of-phrase-in-dependency-tree"><a href="#2-3-Define-the-head-of-phrase-in-dependency-tree" class="headerlink" title="2.3    Define the head of phrase in dependency tree"></a>2.3    Define the head of phrase in dependency tree</h3><p>The standard solution is to use <strong>head rule</strong>: for every non-unary (P)CFG production, designate one RHS nonterminal as containing the head.</p>
<p>S → NP <u>VP,</u> VP → <u>VP</u> PP, PP → P <u>NP</u></p>
<p><strong>The question is how we know which RHS nonterminal is the head?</strong></p>
<h2 id="3-Projectivity"><a href="#3-Projectivity" class="headerlink" title="3.    Projectivity"></a>3.    Projectivity</h2><hr>
<p><strong>Projective arc</strong></p>
<p>An arc from a head to a dependent is said to be projective <u>if there is a path (direct path isn’t neccesary) from the head to <strong>every word</strong> that lies <strong>between</strong> the head and the dependent in the sentence.</u></p>
<p><strong>Projective tree</strong></p>
<p>A dependency tree is then said to be projective if all the arcs that make it up are projective.</p>
<p>Consider the example below:</p>
<p><img src="/Users/shyietliu/Desktop/屏幕快照 2017-10-23 下午10.29.12.png" alt="屏幕快照 2017-10-23 下午10.29.12"></p>
<p>the arc from flight to its modifier was is non-projective since there is no path from flight to the intervening words this and morning.</p>
<h2 id="4-Dependency-Treebanks"><a href="#4-Dependency-Treebanks" class="headerlink" title="4.    Dependency Treebanks"></a>4.    Dependency Treebanks</h2><p>Dependency treebanks have been created using several approaches:</p>
<ul>
<li><p>having human annotators directly generate dependency structures for a given corpus</p>
</li>
<li><p>or using automatic parsers to provide an initial parse and then having annotators hand correct those parsers.</p>
</li>
<li><p>We can also use a deterministic process to translate existing constituent-based treebanks into dependency trees through the use of head rules.</p>
<h2 id="5-Dependency-parsing"><a href="#5-Dependency-parsing" class="headerlink" title="5.Dependency parsing"></a>5.Dependency parsing</h2><h3 id="5-1Transition-based-parsing"><a href="#5-1Transition-based-parsing" class="headerlink" title="5.1Transition-based parsing"></a>5.1Transition-based parsing</h3></li>
</ul>
<p><strong>Given</strong>: a sentence (a sequence of words)</p>
<p><strong>Wanted</strong>: the relation between words (i.e. the dependency tree)</p>
<p>The arc-standard approach parses input sentence $w_1 …w_N$ using two types of reduce actions (three actions altogether):</p>
<ul>
<li>Shift: Read next word $w_i$ from input and push onto the stack.</li>
<li>LeftArc: Assign head-dependent relation $s_2$ ← $s_1$; pop $s_2$</li>
<li>RightArc: Assign head-dependent relation $s_2$ → $s_1$; pop $s_1$</li>
</ul>
<p>where $s_1$ and $s_2$ are the top and second item on the stack, respectively. (So, $s_2$ preceded $s_1$ in the input sentence.)</p>
<p><img src="/Users/shyietliu/Pictures/ANLP-dependency/屏幕快照 2017-10-24 上午2.03.20.png" alt="屏幕快照 2017-10-24 上午2.03.20"></p>
<p>对于在input buffer中的words， 询问Oracle需要执行的指令(shift/left_Arc/right_Arc)，对word进行该指令操作。</p>
<p>Mathematically, we have</p>
<p><img src="/Users/shyietliu/Pictures/ANLP-dependency/屏幕快照 2017-10-24 上午1.56.22.png" alt="屏幕快照 2017-10-24 上午1.56.22"></p>
<p>At each step, the parser consults an oracle (we’ll come back to this shortly) that provides the correct transition operator to use given the current configuration.</p>
<p><strong>How to generate Labelled tree ?</strong></p>
<p>To produce labeled trees, we can parameterize the LEFT- ARC and RIGHTARC operators with dependency labels, as in LEFTARC(NSUBJ) or RIGHTARC(DOBJ).</p>
<h3 id="5-2-differences-from-constituency-parsing"><a href="#5-2-differences-from-constituency-parsing" class="headerlink" title="5.2    differences from constituency parsing"></a>5.2    differences from constituency parsing</h3><ul>
<li>Shift-reduce parser for CFG: not all sequences of actions lead to valid parses. Choose incorrect action → may need to backtrack.</li>
<li>Here, all valid action sequences lead to valid parses.<ul>
<li>Invalid actions: can’t apply LeftArc with root as dependent; can’t apply RightArc with root as head unless input is empty.</li>
<li>Other actions may lead to incorrect parses, but still valid.</li>
</ul>
</li>
<li><p>So, parser doesn’t backtrack. Instead, tries to greedily predict the correct action at each step.</p>
<ul>
<li>Therefore, dependency parsers can be very fast (linear time).</li>
<li>But need a good way to predict correct actions (talking about later).</li>
</ul>
<h3 id="5-3Create-a-oracle"><a href="#5-3Create-a-oracle" class="headerlink" title="5.3Create a oracle"></a>5.3Create a oracle</h3></li>
</ul>
<p>State-of-the-art transition-based systems use <u>supervised machine learning methods</u> to train <strong>classifiers</strong> that play the role of the oracle. Given appropriate training data, these methods learn a function that maps from configurations to transition operators.<br>$$<br>P(action|configuration).<br>$$<br>So what we need now?</p>
<ul>
<li><p>Training data pairs (configurations, transition operators)</p>
</li>
<li><p>Classifiers to predict $$P(action|configuration)$$.</p>
<h4 id="5-3-1Get-training-data"><a href="#5-3-1Get-training-data" class="headerlink" title="5.3.1Get training data"></a>5.3.1Get training data</h4></li>
</ul>
<p>To be more precise, given a reference parse and a configuration, the training oracle proceeds as follows:</p>
<ul>
<li>Choose LEFTARC if it produces a correct head-dependent relation given the reference parse and the current configuration,</li>
<li>Otherwise, choose RIGHTARC if (1) it produces a correct head-dependent re- lation given the reference parse and <u>(2) all of the dependents of the word at the top of the stack have already been assigned,</u></li>
<li>Otherwise, choose SHIFT.</li>
</ul>
<p>The restriction on selecting the RIGHTARC operator is needed to ensure that a word is not popped from the stack, and thus lost to further processing, before all its dependents have been assigned to it.</p>
<p>For example, if we have a set of reference dependency relations:<img src="/Users/shyietliu/Desktop/屏幕截图 2017-10-24 15.07.31.png" alt="屏幕截图 2017-10-24 15.07.31"></p>
<p>We can use the method we discussed above to get training data.</p>
<p><img src="/Users/shyietliu/Desktop/屏幕截图 2017-10-24 15.02.01.png" alt="屏幕截图 2017-10-24 15.02.01"></p>
<h4 id="5-3-2-Feature-templates"><a href="#5-3-2-Feature-templates" class="headerlink" title="5.3.2    Feature templates"></a>5.3.2    Feature templates</h4><p>As we are going to use some classifiers to predict the next action, we need inputs from the current configuration. But what we should use as the feature (the inputs)? Use all the words in input buffer? All words in the stack? And their tags? One way to address this problem is use feature templates as below.</p>
<p>We then choose some features from the current configuration to predict action.</p>
<p><img src="/Users/shyietliu/Desktop/屏幕截图 2017-10-25 01.08.31.png" alt="屏幕截图 2017-10-25 01.08.31"></p>
<h4 id="5-3-3-Use-Classifier-to-predict"><a href="#5-3-3-Use-Classifier-to-predict" class="headerlink" title="5.3.3    Use Classifier to predict"></a>5.3.3    Use Classifier to predict</h4><p>When we have the training data pairs (also the features), we can use many unsupervised classifier to predict the next action.</p>
<p><strong>Generative model</strong></p>
<p>Pass</p>
<p><strong>Discriminative model</strong></p>
<p>Pass</p>
<p>We would discuss a particular model—Max entropy model</p>
<p><strong>MaxEn model</strong></p>
<p>Pass</p>
<p>(没写完…)</p>
<h4 id="5-3-4-An-example-word-sense-disambiguation"><a href="#5-3-4-An-example-word-sense-disambiguation" class="headerlink" title="5.3.4    An example: word sense disambiguation"></a>5.3.4    An example: word sense disambiguation</h4><p>pass</p>
<h3 id="5-4-Bean-search-amp-agenda"><a href="#5-4-Bean-search-amp-agenda" class="headerlink" title="5.4    Bean search &amp; agenda"></a>5.4    Bean search &amp; agenda</h3><h4 id="5-4-1-Bean-search"><a href="#5-4-1-Bean-search" class="headerlink" title="5.4.1    Bean search:"></a>5.4.1    Bean search:</h4><ul>
<li><p>Instead of choosing only the best action at each step, choose a few of the best.</p>
</li>
<li><p>Extend previous partial parses using these options.</p>
</li>
<li><p>At each time step, keep a fixed number of best options, discard anything else.</p>
<h5 id="5-4-1-1-Advantages"><a href="#5-4-1-1-Advantages" class="headerlink" title="5.4.1.1 Advantages:"></a>5.4.1.1 Advantages:</h5></li>
<li><p>May find a better overall parse than greedy search,</p>
</li>
<li><p>While using less time/memory than exhaustive search.</p>
<h4 id="5-4-2Agenda"><a href="#5-4-2Agenda" class="headerlink" title="5.4.2Agenda"></a>5.4.2Agenda</h4></li>
</ul>
<p>An ordered list of configurations (parser state + parse so far)</p>
<p>(没写完…)</p>
<h3 id="5-4-Evaluating-dependency-parsing"><a href="#5-4-Evaluating-dependency-parsing" class="headerlink" title="5.4    Evaluating dependency parsing"></a>5.4    Evaluating dependency parsing</h3><p>By construction, the number of dependencies is the same as the number of words in the sentence. So we do not need to worry about precision and recall, just plain old accuracy.</p>
<ul>
<li>Labelled Attachment Score (LAS): Proportion of words where we predicted the correct head and label.</li>
</ul>
<ul>
<li>Unlabelled Attachment Score (UAS): Proportion of words where we predicted the correct head, regardless of label.</li>
</ul>
<h3 id="5-5-Summary-for-transition-based-parsing"><a href="#5-5-Summary-for-transition-based-parsing" class="headerlink" title="5.5    Summary for transition-based parsing"></a>5.5    Summary for transition-based parsing</h3><p>Arc-standard approach is based on simple shift-reduce idea.</p>
<ul>
<li>Can do labelled or unlabelled parsing, but need to train a classifier to predict next action, as we’ll see next time.</li>
</ul>
<ul>
<li>Greedy algorithm means time complexity is linear in sentence length.</li>
</ul>
<ul>
<li>Only finds projective trees (without special extensions)</li>
<li>Pioneering system: Nivre’s MaltParser.</li>
</ul>
<p>(没写完…)</p>
<h3 id="5-6-Graph-based-parsing"><a href="#5-6-Graph-based-parsing" class="headerlink" title="5.6    Graph-based parsing"></a>5.6    Graph-based parsing</h3><p>Pass</p>

</div>


      </div>
      
  	</div>

    <!-- After footer scripts -->
    <script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
